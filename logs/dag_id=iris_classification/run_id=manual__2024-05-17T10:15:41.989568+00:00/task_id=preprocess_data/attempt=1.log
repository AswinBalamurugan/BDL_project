[2024-05-17T15:45:48.019+0530] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: iris_classification.preprocess_data manual__2024-05-17T10:15:41.989568+00:00 [queued]>
[2024-05-17T15:45:48.021+0530] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: iris_classification.preprocess_data manual__2024-05-17T10:15:41.989568+00:00 [queued]>
[2024-05-17T15:45:48.021+0530] {taskinstance.py:1308} INFO - Starting attempt 1 of 2
[2024-05-17T15:45:48.026+0530] {taskinstance.py:1327} INFO - Executing <Task(PythonOperator): preprocess_data> on 2024-05-17 10:15:41.989568+00:00
[2024-05-17T15:45:48.029+0530] {standard_task_runner.py:57} INFO - Started process 23870 to run task
[2024-05-17T15:45:48.030+0530] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-05-17T15:45:48.033+0530] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'iris_classification', 'preprocess_data', 'manual__2024-05-17T10:15:41.989568+00:00', '--job-id', '6', '--raw', '--subdir', 'DAGS_FOLDER/iris_prediction.py', '--cfg-path', '/var/folders/fy/4q0txy357vvc35bhsxlfyr8c0000gn/T/tmpav22p956']
[2024-05-17T15:45:48.034+0530] {standard_task_runner.py:85} INFO - Job 6: Subtask preprocess_data
[2024-05-17T15:45:48.051+0530] {task_command.py:410} INFO - Running <TaskInstance: iris_classification.preprocess_data manual__2024-05-17T10:15:41.989568+00:00 [running]> on host aswins-macbook-air.local
[2024-05-17T15:45:48.080+0530] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='iris_classification' AIRFLOW_CTX_TASK_ID='preprocess_data' AIRFLOW_CTX_EXECUTION_DATE='2024-05-17T10:15:41.989568+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-05-17T10:15:41.989568+00:00'
[2024-05-17T15:45:50.514+0530] {python.py:183} INFO - Done. Returned value was: (DataFrame[sepal_length: double, sepal_width: double, petal_length: double, petal_width: double, label: string, features: vector], DataFrame[sepal_length: double, sepal_width: double, petal_length: double, petal_width: double, label: string, features: vector])
[2024-05-17T15:45:50.526+0530] {xcom.py:634} ERROR - Object of type tuple is not JSON serializable. If you are using pickle instead of JSON for XCom, then you need to enable pickle support for XCom in your airflow config or make sure to decorate your object with attr.
[2024-05-17T15:45:50.527+0530] {taskinstance.py:1824} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/Users/aswin/anaconda3/envs/bdl/lib/python3.10/site-packages/airflow/utils/json.py", line 91, in default
    return serialize(o)
  File "/Users/aswin/anaconda3/envs/bdl/lib/python3.10/site-packages/airflow/serialization/serde.py", line 144, in serialize
    return encode(classname, version, serialize(data, depth + 1))
  File "/Users/aswin/anaconda3/envs/bdl/lib/python3.10/site-packages/airflow/serialization/serde.py", line 123, in serialize
    return [serialize(d, depth + 1) for d in o]
  File "/Users/aswin/anaconda3/envs/bdl/lib/python3.10/site-packages/airflow/serialization/serde.py", line 123, in <listcomp>
    return [serialize(d, depth + 1) for d in o]
  File "/Users/aswin/anaconda3/envs/bdl/lib/python3.10/site-packages/airflow/serialization/serde.py", line 171, in serialize
    raise TypeError(f"cannot serialize object of type {cls}")
TypeError: cannot serialize object of type <class 'pyspark.sql.dataframe.DataFrame'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/aswin/anaconda3/envs/bdl/lib/python3.10/site-packages/airflow/utils/session.py", line 73, in wrapper
    return func(*args, **kwargs)
  File "/Users/aswin/anaconda3/envs/bdl/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 2354, in xcom_push
    XCom.set(
  File "/Users/aswin/anaconda3/envs/bdl/lib/python3.10/site-packages/airflow/utils/session.py", line 73, in wrapper
    return func(*args, **kwargs)
  File "/Users/aswin/anaconda3/envs/bdl/lib/python3.10/site-packages/airflow/models/xcom.py", line 237, in set
    value = cls.serialize_value(
  File "/Users/aswin/anaconda3/envs/bdl/lib/python3.10/site-packages/airflow/models/xcom.py", line 632, in serialize_value
    return json.dumps(value, cls=XComEncoder).encode("UTF-8")
  File "/Users/aswin/anaconda3/envs/bdl/lib/python3.10/json/__init__.py", line 238, in dumps
    **kw).encode(obj)
  File "/Users/aswin/anaconda3/envs/bdl/lib/python3.10/site-packages/airflow/utils/json.py", line 102, in encode
    o = self.default(o)
  File "/Users/aswin/anaconda3/envs/bdl/lib/python3.10/site-packages/airflow/utils/json.py", line 93, in default
    return super().default(o)
  File "/Users/aswin/anaconda3/envs/bdl/lib/python3.10/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type tuple is not JSON serializable
[2024-05-17T15:45:50.532+0530] {taskinstance.py:1345} INFO - Marking task as UP_FOR_RETRY. dag_id=iris_classification, task_id=preprocess_data, execution_date=20240517T101541, start_date=20240517T101548, end_date=20240517T101550
[2024-05-17T15:45:50.536+0530] {standard_task_runner.py:104} ERROR - Failed to execute job 6 for task preprocess_data (Object of type tuple is not JSON serializable; 23870)
[2024-05-17T15:45:50.550+0530] {local_task_job_runner.py:225} INFO - Task exited with return code 1
[2024-05-17T15:45:50.558+0530] {taskinstance.py:2651} INFO - 0 downstream tasks scheduled from follow-on schedule check
